{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CM7GLVMPbY_f"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import h5py\n",
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e--qiFmPb5f0",
        "outputId": "632b8aa6-9524-48b3-baf5-092b91e0384a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset():\n",
        "    train_dataset = h5py.File('/content/drive/MyDrive/catvnoncat/train_catvnoncat.h5', \"r\")\n",
        "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
        "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
        "\n",
        "    test_dataset = h5py.File('/content/drive/MyDrive/catvnoncat/test_catvnoncat.h5', \"r\")\n",
        "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
        "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
        "\n",
        "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
        "\n",
        "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
        "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
        "\n",
        "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
      ],
      "metadata": {
        "id": "cWHE0yaFeY4-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming load_dataset() function is defined and called to get the dataset\n",
        "train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes = load_dataset()\n",
        "\n",
        "# Split the training set further to create a validation set\n",
        "# Here, we are splitting the training set into 80% training and 20% validation\n",
        "train_x, val_x, train_y, val_y = train_test_split(train_set_x_orig, train_set_y_orig.T, test_size=0.2, random_state=1)\n",
        "train_y = train_y.T\n",
        "val_y = val_y.T"
      ],
      "metadata": {
        "id": "wjS38Ac3fBN2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's say you want to create a smaller prototype dataset, for example with 10% of the data\n",
        "# Here's how you could do it:\n",
        "def create_prototype_dataset(train_x, train_y, val_x, val_y, test_set_x_orig, test_set_y_orig, fraction=0.1):\n",
        "    # Determine the number of examples to include in the prototype\n",
        "    num_train = int(train_x.shape[0] * fraction)\n",
        "    num_val = int(val_x.shape[0] * fraction)\n",
        "    num_test = int(test_set_x_orig.shape[0] * fraction)\n",
        "\n",
        "    # Create prototype datasets\n",
        "    prototype_train_x = train_x[:num_train]\n",
        "    prototype_train_y = train_y[:, :num_train]\n",
        "    prototype_val_x = val_x[:num_val]\n",
        "    prototype_val_y = val_y[:, :num_val]\n",
        "    prototype_test_x = test_set_x_orig[:num_test]\n",
        "    prototype_test_y = test_set_y_orig[:, :num_test]\n",
        "\n",
        "    return prototype_train_x, prototype_train_y, prototype_val_x, prototype_val_y, prototype_test_x, prototype_test_y"
      ],
      "metadata": {
        "id": "_m5m18FYh6jl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function to create the prototype datasets\n",
        "proto_train_x, proto_train_y, proto_val_x, proto_val_y, proto_test_x, proto_test_y = create_prototype_dataset(train_x, train_y, val_x, val_y, test_set_x_orig, test_set_y_orig)\n",
        "\n",
        "# Now you have a prototype dataset with a smaller number of examples\n",
        "print(f\"Prototype Training set: {proto_train_x.shape}, {proto_train_y.shape}\")\n",
        "print(f\"Prototype Validation set: {proto_val_x.shape}, {proto_val_y.shape}\")\n",
        "print(f\"Prototype Testing set: {proto_test_x.shape}, {proto_test_y.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3ItHEfnjNbK",
        "outputId": "ac38d74f-fc17-40ae-96a2-437ed2f024bb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prototype Training set: (16, 64, 64, 3), (1, 16)\n",
            "Prototype Validation set: (4, 64, 64, 3), (1, 4)\n",
            "Prototype Testing set: (5, 64, 64, 3), (1, 5)\n"
          ]
        }
      ]
    }
  ]
}