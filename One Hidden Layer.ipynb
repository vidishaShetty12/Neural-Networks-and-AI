{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "110ea0da",
   "metadata": {},
   "source": [
    "### One Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24a8d3f",
   "metadata": {},
   "source": [
    "#### Developing a Neural Network with one hidden layer, Using object-oriented approach.\n",
    "#### Class structure:\n",
    "       Class – Activation\n",
    "       Class – Neuron\n",
    "       Class – Layer\n",
    "       Class – Parameters\n",
    "       Class – Model (start with Neural Network with one hidden layer\n",
    "       Class – LossFunction\n",
    "       Class – ForwardProp\n",
    "       Class – BackProp\n",
    "       Class – GradDescent\n",
    "       Class – Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e09862ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d3d8c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(x):\n",
    "        return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2e31362",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neuron\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        self.output = None\n",
    "        self.input_derivative = None\n",
    "        \n",
    "#Layer\n",
    "class Layer:\n",
    "    def __init__(self, neurons):\n",
    "        self.neurons = neurons\n",
    "        \n",
    "#Parameters\n",
    "class Parameters:\n",
    "    @staticmethod\n",
    "    def initialize(input_size, hidden_size, output_size):\n",
    "        hidden_layer = [Neuron(np.random.randn(input_size), np.random.randn()) for _ in range(hidden_size)]\n",
    "        output_layer = [Neuron(np.random.randn(hidden_size), np.random.randn()) for _ in range(output_size)]\n",
    "        return Layer(hidden_layer), Layer(output_layer)\n",
    "    \n",
    "#Model\n",
    "class Model:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.hidden_layer, self.output_layer = Parameters.initialize(input_size, hidden_size, output_size)\n",
    "\n",
    "#Loss Function\n",
    "class LossFunction:\n",
    "    @staticmethod\n",
    "    def mean_squared_error(predictions, targets):\n",
    "        return np.mean((predictions - targets) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_squared_error_derivative(predictions, targets):\n",
    "        return 2 * (predictions - targets) / targets.size\n",
    "    \n",
    "#Forward Propagation\n",
    "class ForwardProp:\n",
    "    @staticmethod\n",
    "    def run(layer, inputs):\n",
    "        outputs = []\n",
    "        for neuron in layer.neurons:\n",
    "            total_input = np.dot(inputs, neuron.weights) + neuron.bias\n",
    "            neuron.output = Activation.sigmoid(total_input)\n",
    "            outputs.append(neuron.output)\n",
    "        return np.array(outputs)\n",
    "\n",
    "#back Propagation\n",
    "class BackProp:\n",
    "    @staticmethod\n",
    "    def run(layer, inputs, output_derivative):\n",
    "        input_derivative = np.zeros_like(inputs)\n",
    "        for i, neuron in enumerate(layer.neurons):\n",
    "            neuron_derivative = output_derivative[i] * Activation.sigmoid_derivative(neuron.output)\n",
    "            neuron.input_derivative = neuron_derivative * inputs\n",
    "            input_derivative += neuron.weights * neuron_derivative\n",
    "        return input_derivative\n",
    "\n",
    "#gradient Descent\n",
    "class GradDescent:\n",
    "    @staticmethod\n",
    "    def update_weights(layer, learning_rate):\n",
    "        for neuron in layer.neurons:\n",
    "            neuron.weights -= learning_rate * neuron.input_derivative\n",
    "            neuron.bias -= learning_rate * np.mean(neuron.input_derivative)\n",
    "\n",
    "#Training\n",
    "class Training:\n",
    "    @staticmethod\n",
    "    def train(model, inputs, targets, epochs, learning_rate):\n",
    "        for _ in range(epochs):\n",
    "            # Forward pass\n",
    "            hidden_output = ForwardProp.run(model.hidden_layer, inputs)\n",
    "            final_output = ForwardProp.run(model.output_layer, hidden_output)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = LossFunction.mean_squared_error(final_output, targets)\n",
    "            print(f\"Loss: {loss}\")\n",
    "\n",
    "            # Backward pass\n",
    "            output_error = LossFunction.mean_squared_error_derivative(final_output, targets)\n",
    "            hidden_error = BackProp.run(model.output_layer, hidden_output, output_error)\n",
    "            BackProp.run(model.hidden_layer, inputs, hidden_error)\n",
    "\n",
    "            # Update weights\n",
    "            GradDescent.update_weights(model.output_layer, learning_rate)\n",
    "            GradDescent.update_weights(model.hidden_layer, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77dc1d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.34708659728457325\n",
      "Loss: 0.344975072868928\n",
      "Loss: 0.34287118922423604\n",
      "Loss: 0.3407750703705276\n",
      "Loss: 0.33868683777259284\n",
      "Loss: 0.3366066103118876\n",
      "Loss: 0.33453450426048026\n",
      "Loss: 0.33247063325703763\n",
      "Loss: 0.33041510828484855\n",
      "Loss: 0.32836803765187916\n",
      "Loss: 0.32632952697285345\n",
      "Loss: 0.3242996791533454\n",
      "Loss: 0.3222785943758727\n",
      "Loss: 0.3202663700879744\n",
      "Loss: 0.31826310099225547\n",
      "Loss: 0.31626887903837725\n",
      "Loss: 0.31428379341697105\n",
      "Loss: 0.3123079305554525\n",
      "Loss: 0.3103413741157035\n",
      "Loss: 0.3083842049936021\n",
      "Loss: 0.30643650132036176\n",
      "Loss: 0.30449833846565044\n",
      "Loss: 0.30256978904245646\n",
      "Loss: 0.3006509229136626\n",
      "Loss: 0.29874180720029453\n",
      "Loss: 0.2968425062913998\n",
      "Loss: 0.2949530818555239\n",
      "Loss: 0.29307359285373624\n",
      "Loss: 0.29120409555416654\n",
      "Loss: 0.2893446435480086\n",
      "Loss: 0.28749528776694633\n",
      "Loss: 0.28565607650195607\n",
      "Loss: 0.2838270554234417\n",
      "Loss: 0.2820082676026551\n",
      "Loss: 0.280199753534353\n",
      "Loss: 0.2784015511606475\n",
      "Loss: 0.2766136958959976\n",
      "Loss: 0.2748362206532982\n",
      "Loss: 0.2730691558710141\n",
      "Loss: 0.2713125295413166\n",
      "Loss: 0.26956636723916705\n",
      "Loss: 0.2678306921523073\n",
      "Loss: 0.2661055251121022\n",
      "Loss: 0.2643908846251933\n",
      "Loss: 0.26268678690590774\n",
      "Loss: 0.2609932459093855\n",
      "Loss: 0.2593102733653689\n",
      "Loss: 0.2576378788126163\n",
      "Loss: 0.2559760696338877\n",
      "Loss: 0.25432485109146313\n",
      "Loss: 0.2526842263631451\n",
      "Loss: 0.25105419657870387\n",
      "Loss: 0.24943476085672342\n",
      "Loss: 0.24782591634180365\n",
      "Loss: 0.24622765824207904\n",
      "Loss: 0.2446399798670146\n",
      "Loss: 0.243062872665438\n",
      "Loss: 0.24149632626376893\n",
      "Loss: 0.23994032850441135\n",
      "Loss: 0.238394865484268\n",
      "Loss: 0.23685992159334435\n",
      "Loss: 0.23533547955340797\n",
      "Loss: 0.2338215204566666\n",
      "Loss: 0.2323180238044357\n",
      "Loss: 0.23082496754576265\n",
      "Loss: 0.22934232811597846\n",
      "Loss: 0.22787008047514504\n",
      "Loss: 0.22640819814637428\n",
      "Loss: 0.22495665325398623\n",
      "Loss: 0.2235154165614866\n",
      "Loss: 0.22208445750933367\n",
      "Loss: 0.22066374425247276\n",
      "Loss: 0.21925324369761603\n",
      "Loss: 0.21785292154024422\n",
      "Loss: 0.2164627423013117\n",
      "Loss: 0.2150826693636327\n",
      "Loss: 0.21371266500793254\n",
      "Loss: 0.21235269044854407\n",
      "Loss: 0.21100270586873482\n",
      "Loss: 0.20966267045564668\n",
      "Loss: 0.208332542434836\n",
      "Loss: 0.2070122791043985\n",
      "Loss: 0.205701836868666\n",
      "Loss: 0.20440117127146565\n",
      "Loss: 0.20311023702892678\n",
      "Loss: 0.20182898806182772\n",
      "Loss: 0.20055737752747338\n",
      "Loss: 0.19929535785109284\n",
      "Loss: 0.19804288075675178\n",
      "Loss: 0.19679989729777062\n",
      "Loss: 0.1955663578866439\n",
      "Loss: 0.1943422123244543\n",
      "Loss: 0.19312740982977614\n",
      "Loss: 0.1919218990670664\n",
      "Loss: 0.19072562817453673\n",
      "Loss: 0.18953854479150625\n",
      "Loss: 0.1883605960852313\n",
      "Loss: 0.18719172877721102\n",
      "Loss: 0.18603188916896865\n",
      "Loss: 0.18488102316730587\n",
      "Loss: 0.18373907630903216\n",
      "Loss: 0.1826059937851691\n",
      "Loss: 0.1814817204646297\n",
      "Loss: 0.180366200917376\n",
      "Loss: 0.17925937943705436\n",
      "Loss: 0.17816120006311356\n",
      "Loss: 0.17707160660240528\n",
      "Loss: 0.17599054265027322\n",
      "Loss: 0.17491795161113116\n",
      "Loss: 0.17385377671853575\n",
      "Loss: 0.1727979610547579\n",
      "Loss: 0.17175044756985552\n",
      "Loss: 0.17071117910025374\n",
      "Loss: 0.16968009838683717\n",
      "Loss: 0.1686571480925596\n",
      "Loss: 0.16764227081957347\n",
      "Loss: 0.16663540912589125\n",
      "Loss: 0.16563650554157636\n",
      "Loss: 0.16464550258447558\n",
      "Loss: 0.16366234277549674\n",
      "Loss: 0.16268696865343674\n",
      "Loss: 0.16171932278937007\n",
      "Loss: 0.16075934780059864\n",
      "Loss: 0.15980698636417504\n",
      "Loss: 0.15886218123000223\n",
      "Loss: 0.15792487523351828\n",
      "Loss: 0.15699501130797164\n",
      "Loss: 0.15607253249629546\n",
      "Loss: 0.15515738196258638\n",
      "Loss: 0.15424950300319507\n",
      "Loss: 0.1533488390574368\n",
      "Loss: 0.15245533371792744\n",
      "Loss: 0.1515689307405527\n",
      "Loss: 0.15068957405407804\n",
      "Loss: 0.14981720776940496\n",
      "Loss: 0.1489517761884836\n",
      "Loss: 0.14809322381288417\n",
      "Loss: 0.14724149535203884\n",
      "Loss: 0.14639653573115877\n",
      "Loss: 0.14555829009883256\n",
      "Loss: 0.14472670383431552\n",
      "Loss: 0.1439017225545138\n",
      "Loss: 0.14308329212067297\n",
      "Loss: 0.14227135864477553\n",
      "Loss: 0.1414658684956551\n",
      "Loss: 0.14066676830483457\n",
      "Loss: 0.13987400497209354\n",
      "Loss: 0.13908752567077234\n",
      "Loss: 0.13830727785281888\n",
      "Loss: 0.13753320925358525\n",
      "Loss: 0.13676526789637902\n",
      "Loss: 0.13600340209677722\n",
      "Loss: 0.13524756046670763\n",
      "Loss: 0.13449769191830513\n",
      "Loss: 0.13375374566754636\n",
      "Loss: 0.13301567123767244\n",
      "Loss: 0.13228341846240127\n",
      "Loss: 0.13155693748893896\n",
      "Loss: 0.1308361787807933\n",
      "Loss: 0.1301210931203964\n",
      "Loss: 0.12941163161154132\n",
      "Loss: 0.12870774568163684\n",
      "Loss: 0.1280093870837896\n",
      "Loss: 0.12731650789871238\n",
      "Loss: 0.12662906053647102\n",
      "Loss: 0.12594699773806783\n",
      "Loss: 0.12527027257687212\n",
      "Loss: 0.12459883845989844\n",
      "Loss: 0.12393264912894018\n",
      "Loss: 0.12327165866156163\n",
      "Loss: 0.12261582147195257\n",
      "Loss: 0.12196509231165122\n",
      "Loss: 0.12131942627013895\n",
      "Loss: 0.12067877877531102\n",
      "Loss: 0.1200431055938277\n",
      "Loss: 0.1194123628313496\n",
      "Loss: 0.11878650693266135\n",
      "Loss: 0.11816549468168744\n",
      "Loss: 0.11754928320140345\n",
      "Loss: 0.11693782995364788\n",
      "Loss: 0.11633109273883575\n",
      "Loss: 0.11572902969558012\n",
      "Loss: 0.11513159930022289\n",
      "Loss: 0.11453876036627929\n",
      "Loss: 0.113950472043799\n",
      "Loss: 0.11336669381864722\n",
      "Loss: 0.11278738551170837\n",
      "Loss: 0.11221250727801643\n",
      "Loss: 0.11164201960581352\n",
      "Loss: 0.11107588331554008\n",
      "Loss: 0.11051405955876098\n",
      "Loss: 0.10995650981702698\n",
      "Loss: 0.10940319590067785\n",
      "Loss: 0.10885407994758689\n",
      "Loss: 0.10830912442185078\n",
      "Loss: 0.10776829211242735\n",
      "Loss: 0.10723154613172187\n",
      "Loss: 0.10669884991412758\n",
      "Loss: 0.10617016721451816\n",
      "Loss: 0.1056454621066984\n",
      "Loss: 0.10512469898181315\n",
      "Loss: 0.10460784254671654\n",
      "Loss: 0.10409485782230417\n",
      "Loss: 0.10358571014181014\n",
      "Loss: 0.10308036514907\n",
      "Loss: 0.10257878879675267\n",
      "Loss: 0.10208094734456212\n",
      "Loss: 0.10158680735741074\n",
      "Loss: 0.10109633570356606\n",
      "Loss: 0.10060949955277347\n",
      "Loss: 0.10012626637435397\n",
      "Loss: 0.09964660393528145\n",
      "Loss: 0.0991704802982393\n",
      "Loss: 0.0986978638196569\n",
      "Loss: 0.09822872314772997\n",
      "Loss: 0.09776302722042408\n",
      "Loss: 0.09730074526346279\n",
      "Loss: 0.09684184678830232\n",
      "Loss: 0.09638630159009379\n",
      "Loss: 0.0959340797456336\n",
      "Loss: 0.09548515161130405\n",
      "Loss: 0.0950394878210042\n",
      "Loss: 0.09459705928407322\n",
      "Loss: 0.09415783718320567\n",
      "Loss: 0.0937217929723613\n",
      "Loss: 0.0932888983746697\n",
      "Loss: 0.09285912538032957\n",
      "Loss: 0.09243244624450576\n",
      "Loss: 0.09200883348522292\n",
      "Loss: 0.0915882598812574\n",
      "Loss: 0.09117069847002839\n",
      "Loss: 0.09075612254548836\n",
      "Loss: 0.09034450565601412\n",
      "Loss: 0.08993582160229907\n",
      "Loss: 0.08953004443524702\n",
      "Loss: 0.08912714845386865\n",
      "Loss: 0.08872710820318039\n",
      "Loss: 0.088329898472108\n",
      "Loss: 0.08793549429139332\n",
      "Loss: 0.08754387093150622\n",
      "Loss: 0.08715500390056162\n",
      "Loss: 0.08676886894224266\n",
      "Loss: 0.08638544203372914\n",
      "Loss: 0.08600469938363403\n",
      "Loss: 0.08562661742994547\n",
      "Loss: 0.08525117283797805\n",
      "Loss: 0.08487834249833022\n",
      "Loss: 0.08450810352485148\n",
      "Loss: 0.08414043325261751\n",
      "Loss: 0.0837753092359144\n",
      "Loss: 0.08341270924623213\n",
      "Loss: 0.08305261127026801\n",
      "Loss: 0.08269499350794\n",
      "Loss: 0.08233983437040991\n",
      "Loss: 0.08198711247811716\n",
      "Loss: 0.08163680665882438\n",
      "Loss: 0.08128889594567158\n",
      "Loss: 0.08094335957524447\n",
      "Loss: 0.08060017698565203\n",
      "Loss: 0.08025932781461635\n",
      "Loss: 0.07992079189757471\n",
      "Loss: 0.0795845492657932\n",
      "Loss: 0.07925058014449232\n",
      "Loss: 0.07891886495098559\n",
      "Loss: 0.07858938429282974\n",
      "Loss: 0.07826211896598834\n",
      "Loss: 0.07793704995300722\n",
      "Loss: 0.07761415842120385\n",
      "Loss: 0.07729342572086827\n",
      "Loss: 0.07697483338347828\n",
      "Loss: 0.07665836311992726\n",
      "Loss: 0.07634399681876469\n",
      "Loss: 0.0760317165444508\n",
      "Loss: 0.07572150453562417\n",
      "Loss: 0.07541334320338253\n",
      "Loss: 0.07510721512957709\n",
      "Loss: 0.07480310306512059\n",
      "Loss: 0.07450098992830838\n",
      "Loss: 0.07420085880315405\n",
      "Loss: 0.07390269293773706\n",
      "Loss: 0.07360647574256524\n",
      "Loss: 0.07331219078895053\n",
      "Loss: 0.07301982180739837\n",
      "Loss: 0.07272935268601005\n",
      "Loss: 0.07244076746889996\n",
      "Loss: 0.07215405035462502\n",
      "Loss: 0.07186918569462884\n",
      "Loss: 0.07158615799169886\n",
      "Loss: 0.07130495189843696\n",
      "Loss: 0.07102555221574408\n",
      "Loss: 0.07074794389131804\n",
      "Loss: 0.07047211201816451\n",
      "Loss: 0.07019804183312219\n",
      "Loss: 0.06992571871540071\n",
      "Loss: 0.06965512818513193\n",
      "Loss: 0.06938625590193528\n",
      "Loss: 0.06911908766349505\n",
      "Loss: 0.06885360940415207\n",
      "Loss: 0.06858980719350823\n",
      "Loss: 0.0683276672350437\n",
      "Loss: 0.06806717586474725\n",
      "Loss: 0.06780831954976022\n",
      "Loss: 0.06755108488703268\n",
      "Loss: 0.06729545860199261\n",
      "Loss: 0.06704142754722803\n",
      "Loss: 0.0667889787011817\n",
      "Loss: 0.06653809916685872\n",
      "Loss: 0.06628877617054586\n",
      "Loss: 0.06604099706054485\n",
      "Loss: 0.06579474930591646\n",
      "Loss: 0.0655500204952382\n",
      "Loss: 0.06530679833537374\n",
      "Loss: 0.06506507065025403\n",
      "Loss: 0.06482482537967188\n",
      "Loss: 0.06458605057808722\n",
      "Loss: 0.06434873441344467\n",
      "Loss: 0.06411286516600351\n",
      "Loss: 0.0638784312271788\n",
      "Loss: 0.06364542109839465\n",
      "Loss: 0.06341382338994825\n",
      "Loss: 0.06318362681988679\n",
      "Loss: 0.0629548202128945\n",
      "Loss: 0.0627273924991917\n",
      "Loss: 0.06250133271344505\n",
      "Loss: 0.06227662999368878\n",
      "Loss: 0.062053273580257196\n",
      "Loss: 0.06183125281472782\n",
      "Loss: 0.061610557138876254\n",
      "Loss: 0.06139117609364051\n",
      "Loss: 0.0611730993180974\n",
      "Loss: 0.060956316548448665\n",
      "Loss: 0.06074081761701827\n",
      "Loss: 0.06052659245125953\n",
      "Loss: 0.06031363107277282\n",
      "Loss: 0.060101923596334164\n",
      "Loss: 0.05989146022893288\n",
      "Loss: 0.05968223126882022\n",
      "Loss: 0.05947422710456743\n",
      "Loss: 0.05926743821413437\n",
      "Loss: 0.05906185516394718\n",
      "Loss: 0.05885746860798615\n",
      "Loss: 0.058654269286883054\n",
      "Loss: 0.05845224802702844\n",
      "Loss: 0.05825139573968765\n",
      "Loss: 0.058051703420126555\n",
      "Loss: 0.05785316214674717\n",
      "Loss: 0.05765576308023126\n",
      "Loss: 0.05745949746269416\n",
      "Loss: 0.057264356616846895\n",
      "Loss: 0.05707033194516774\n",
      "Loss: 0.056877414929082076\n",
      "Loss: 0.05668559712815154\n",
      "Loss: 0.05649487017927168\n",
      "Loss: 0.05630522579587809\n",
      "Loss: 0.056116655767160864\n",
      "Loss: 0.05592915195728805\n",
      "Loss: 0.055742706304637006\n",
      "Loss: 0.055557310821033955\n",
      "Loss: 0.05537295759100221\n",
      "Loss: 0.05518963877101761\n",
      "Loss: 0.055007346588773084\n",
      "Loss: 0.0548260733424501\n",
      "Loss: 0.05464581139999858\n",
      "Loss: 0.05446655319842443\n",
      "Loss: 0.054288291243084694\n",
      "Loss: 0.05411101810699034\n",
      "Loss: 0.0539347264301166\n",
      "Loss: 0.05375940891872064\n",
      "Loss: 0.053585058344666966\n",
      "Loss: 0.053411667544759435\n",
      "Loss: 0.05323922942008136\n",
      "Loss: 0.05306773693534197\n",
      "Loss: 0.05289718311823035\n",
      "Loss: 0.05272756105877654\n",
      "Loss: 0.05255886390871878\n",
      "Loss: 0.05239108488087869\n",
      "Loss: 0.0522242172485423\n",
      "Loss: 0.052058254344848315\n",
      "Loss: 0.051893189562182605\n",
      "Loss: 0.051729016351579955\n",
      "Loss: 0.0515657282221311\n",
      "Loss: 0.05140331874039762\n",
      "Loss: 0.05124178152983208\n",
      "Loss: 0.05108111027020497\n",
      "Loss: 0.050921298697037556\n",
      "Loss: 0.050762340601041345\n",
      "Loss: 0.05060422982756307\n",
      "Loss: 0.050446960276036096\n",
      "Loss: 0.05029052589943755\n",
      "Loss: 0.050134920703751536\n",
      "Loss: 0.0499801387474379\n",
      "Loss: 0.0498261741409068\n",
      "Loss: 0.049673021045999625\n",
      "Loss: 0.049520673675474115\n",
      "Loss: 0.04936912629249672\n",
      "Loss: 0.049218373210139034\n",
      "Loss: 0.04906840879088096\n",
      "Loss: 0.0489192274461179\n",
      "Loss: 0.04877082363567448\n",
      "Loss: 0.048623191867322814\n",
      "Loss: 0.048476326696306304\n",
      "Loss: 0.04833022272486853\n",
      "Loss: 0.04818487460178695\n",
      "Loss: 0.04804027702191215\n",
      "Loss: 0.04789642472571176\n",
      "Loss: 0.0477533124988193\n",
      "Loss: 0.04761093517158783\n",
      "Loss: 0.04746928761864893\n",
      "Loss: 0.047328364758475565\n",
      "Loss: 0.04718816155295042\n",
      "Loss: 0.04704867300693875\n",
      "Loss: 0.04690989416786533\n",
      "Loss: 0.046771820125296626\n",
      "Loss: 0.04663444601052725\n",
      "Loss: 0.04649776699617035\n",
      "Loss: 0.04636177829575357\n",
      "Loss: 0.04622647516331802\n",
      "Loss: 0.046091852893022485\n",
      "Loss: 0.045957906818751566\n",
      "Loss: 0.045824632313727943\n",
      "Loss: 0.045692024790129286\n",
      "Loss: 0.045560079698708296\n",
      "Loss: 0.04542879252841802\n",
      "Loss: 0.045298158806040274\n",
      "Loss: 0.04516817409581847\n",
      "Loss: 0.04503883399909436\n",
      "Loss: 0.04491013415394837\n",
      "Loss: 0.04478207023484432\n",
      "Loss: 0.04465463795227745\n",
      "Loss: 0.04452783305242634\n",
      "Loss: 0.04440165131680865\n",
      "Loss: 0.044276088561940444\n",
      "Loss: 0.044151140638999546\n",
      "Loss: 0.04402680343349147\n",
      "Loss: 0.04390307286492006\n",
      "Loss: 0.04377994488646108\n",
      "Loss: 0.04365741548463913\n",
      "Loss: 0.04353548067900855\n",
      "Loss: 0.043414136521836906\n",
      "Loss: 0.04329337909779301\n",
      "Loss: 0.04317320452363669\n",
      "Loss: 0.04305360894791351\n",
      "Loss: 0.042934588550651395\n",
      "Loss: 0.04281613954306135\n",
      "Loss: 0.042698258167240634\n",
      "Loss: 0.04258094069587995\n",
      "Loss: 0.04246418343197286\n",
      "Loss: 0.042347982708528555\n",
      "Loss: 0.042232334888288224\n",
      "Loss: 0.04211723636344336\n",
      "Loss: 0.04200268355535808\n",
      "Loss: 0.0418886729142934\n",
      "Loss: 0.04177520091913556\n",
      "Loss: 0.041662264077125984\n",
      "Loss: 0.041549858923595\n",
      "Loss: 0.041437982021697704\n",
      "Loss: 0.041326629962153305\n",
      "Loss: 0.0412157993629863\n",
      "Loss: 0.04110548686927142\n",
      "Loss: 0.040995689152880266\n",
      "Loss: 0.04088640291223114\n",
      "Loss: 0.040777624872041465\n",
      "Loss: 0.04066935178308258\n",
      "Loss: 0.0405615804219373\n",
      "Loss: 0.040454307590759755\n",
      "Loss: 0.04034753011703807\n",
      "Loss: 0.04024124485335941\n",
      "Loss: 0.04013544867717708\n",
      "Loss: 0.04003013849058064\n",
      "Loss: 0.03992531122006796\n",
      "Loss: 0.039820963816319964\n",
      "Loss: 0.03971709325397743\n",
      "Loss: 0.03961369653142044\n",
      "Loss: 0.03951077067054953\n",
      "Loss: 0.039408312716569874\n",
      "Loss: 0.039306319737777005\n",
      "Loss: 0.03920478882534527\n",
      "Loss: 0.03910371709311804\n",
      "Loss: 0.0390031016774004\n",
      "Loss: 0.03890293973675406\n",
      "Loss: 0.038803228451793445\n",
      "Loss: 0.03870396502498574\n",
      "Loss: 0.03860514668045084\n",
      "Loss: 0.03850677066376481\n",
      "Loss: 0.038408834241765055\n",
      "Loss: 0.038311334702356994\n",
      "Loss: 0.03821426935432344\n",
      "Loss: 0.03811763552713525\n",
      "Loss: 0.03802143057076433\n",
      "Loss: 0.03792565185549857\n",
      "Loss: 0.0378302967717584\n",
      "Loss: 0.03773536272991524\n",
      "Loss: 0.037640847160112324\n",
      "Loss: 0.03754674751208619\n",
      "Loss: 0.037453061254991704\n",
      "Loss: 0.03735978587722685\n",
      "Loss: 0.03726691888626123\n",
      "Loss: 0.037174457808464734\n",
      "Loss: 0.03708240018893897\n",
      "Loss: 0.03699074359134984\n",
      "Loss: 0.03689948559776211\n",
      "Loss: 0.036808623808475646\n",
      "Loss: 0.03671815584186286\n",
      "Loss: 0.03662807933420841\n",
      "Loss: 0.0365383919395503\n",
      "Loss: 0.036449091329522086\n",
      "Loss: 0.03636017519319765\n",
      "Loss: 0.03627164123693651\n",
      "Loss: 0.03618348718423133\n",
      "Loss: 0.036095710775556906\n",
      "Loss: 0.03600830976822019\n",
      "Loss: 0.03592128193621254\n",
      "Loss: 0.03583462507006267\n",
      "Loss: 0.03574833697669185\n",
      "Loss: 0.035662415479269766\n",
      "Loss: 0.03557685841707238\n",
      "Loss: 0.035491663645341114\n",
      "Loss: 0.035406829035143236\n",
      "Loss: 0.03532235247323363\n",
      "Loss: 0.035238231861918295\n",
      "Loss: 0.03515446511891876\n",
      "Loss: 0.03507105017723808\n",
      "Loss: 0.0349879849850281\n",
      "Loss: 0.03490526750545796\n",
      "Loss: 0.034822895716584194\n",
      "Loss: 0.034740867611221735\n",
      "Loss: 0.034659181196816095\n",
      "Loss: 0.03457783449531757\n",
      "Loss: 0.03449682554305588\n",
      "Loss: 0.03441615239061609\n",
      "Loss: 0.03433581310271644\n",
      "Loss: 0.03425580575808655\n",
      "Loss: 0.03417612844934745\n",
      "Loss: 0.034096779282892134\n",
      "Loss: 0.03401775637876813\n",
      "Loss: 0.03393905787056028\n",
      "Loss: 0.03386068190527566\n",
      "Loss: 0.03378262664322829\n",
      "Loss: 0.03370489025792661\n",
      "Loss: 0.03362747093596076\n",
      "Loss: 0.033550366876891315\n",
      "Loss: 0.033473576293139365\n",
      "Loss: 0.03339709740987744\n",
      "Loss: 0.0333209284649216\n",
      "Loss: 0.03324506770862429\n",
      "Loss: 0.03316951340376843\n",
      "Loss: 0.03309426382546279\n",
      "Loss: 0.033019317261037816\n",
      "Loss: 0.032944672009943006\n",
      "Loss: 0.03287032638364471\n",
      "Loss: 0.03279627870552567\n",
      "Loss: 0.03272252731078449\n",
      "Loss: 0.03264907054633718\n",
      "Loss: 0.03257590677071873\n",
      "Loss: 0.032503034353986465\n",
      "Loss: 0.03243045167762323\n",
      "Loss: 0.032358157134442796\n",
      "Loss: 0.03228614912849504\n",
      "Loss: 0.032214426074972685\n",
      "Loss: 0.032142986400118846\n",
      "Loss: 0.032071828541135085\n",
      "Loss: 0.03200095094609082\n",
      "Loss: 0.031930352073833275\n",
      "Loss: 0.031860030393898536\n",
      "Loss: 0.03178998438642298\n",
      "Loss: 0.03172021254205613\n",
      "Loss: 0.03165071336187411\n",
      "Loss: 0.031581485357293784\n",
      "Loss: 0.03151252704998743\n",
      "Loss: 0.031443836971799255\n",
      "Loss: 0.031375413664661476\n",
      "Loss: 0.03130725568051201\n",
      "Loss: 0.031239361581212547\n",
      "Loss: 0.031171729938467514\n",
      "Loss: 0.031104359333743805\n",
      "Loss: 0.031037248358191192\n",
      "Loss: 0.03097039561256348\n",
      "Loss: 0.030903799707140873\n",
      "Loss: 0.030837459261651903\n",
      "Loss: 0.030771372905197422\n",
      "Loss: 0.030705539276174407\n",
      "Loss: 0.030639957022200976\n",
      "Loss: 0.03057462480004167\n",
      "Loss: 0.030509541275533673\n",
      "Loss: 0.030444705123513938\n",
      "Loss: 0.030380115027746492\n",
      "Loss: 0.030315769680850695\n",
      "Loss: 0.03025166778423026\n",
      "Loss: 0.030187808048002543\n",
      "Loss: 0.030124189190929015\n",
      "Loss: 0.030060809940346017\n",
      "Loss: 0.029997669032096143\n",
      "Loss: 0.02993476521046029\n",
      "Loss: 0.02987209722809076\n",
      "Loss: 0.029809663845943943\n",
      "Loss: 0.029747463833214978\n",
      "Loss: 0.029685495967271724\n",
      "Loss: 0.02962375903359033\n",
      "Loss: 0.02956225182569086\n",
      "Loss: 0.029500973145073328\n",
      "Loss: 0.029439921801155056\n",
      "Loss: 0.02937909661120788\n",
      "Loss: 0.02931849640029634\n",
      "Loss: 0.02925812000121613\n",
      "Loss: 0.029197966254433515\n",
      "Loss: 0.02913803400802471\n",
      "Loss: 0.029078322117616403\n",
      "Loss: 0.02901882944632664\n",
      "Loss: 0.02895955486470588\n",
      "Loss: 0.028900497250679016\n",
      "Loss: 0.0288416554894878\n",
      "Loss: 0.028783028473633673\n",
      "Loss: 0.02872461510282128\n",
      "Loss: 0.02866641428390235\n",
      "Loss: 0.028608424930819963\n",
      "Loss: 0.028550645964553905\n",
      "Loss: 0.028493076313065715\n",
      "Loss: 0.028435714911244695\n",
      "Loss: 0.028378560700854292\n",
      "Loss: 0.028321612630479182\n",
      "Loss: 0.0282648696554723\n",
      "Loss: 0.028208330737902933\n",
      "Loss: 0.0281519948465047\n",
      "Loss: 0.028095860956624523\n",
      "Loss: 0.02803992805017183\n",
      "Loss: 0.02798419511556812\n",
      "Loss: 0.027928661147696973\n",
      "Loss: 0.027873325147854754\n",
      "Loss: 0.027818186123701495\n",
      "Loss: 0.02776324308921226\n",
      "Loss: 0.027708495064628948\n",
      "Loss: 0.027653941076412808\n",
      "Loss: 0.027599580157196733\n",
      "Loss: 0.02754541134573863\n",
      "Loss: 0.027491433686874716\n",
      "Loss: 0.027437646231473666\n",
      "Loss: 0.027384048036390732\n",
      "Loss: 0.027330638164422424\n",
      "Loss: 0.027277415684261834\n",
      "Loss: 0.02722437967045402\n",
      "Loss: 0.027171529203351752\n",
      "Loss: 0.02711886336907196\n",
      "Loss: 0.027066381259452342\n",
      "Loss: 0.02701408197200846\n",
      "Loss: 0.026961964609890934\n",
      "Loss: 0.02691002828184343\n",
      "Loss: 0.026858272102160696\n",
      "Loss: 0.026806695190647038\n",
      "Loss: 0.02675529667257527\n",
      "Loss: 0.026704075678645817\n",
      "Loss: 0.026653031344946374\n",
      "Loss: 0.026602162812911705\n",
      "Loss: 0.02655146922928408\n",
      "Loss: 0.026500949746073667\n",
      "Loss: 0.026450603520519617\n",
      "Loss: 0.02640042971505125\n",
      "Loss: 0.02635042749724973\n",
      "Loss: 0.026300596039809943\n",
      "Loss: 0.026250934520502745\n",
      "Loss: 0.02620144212213763\n",
      "Loss: 0.026152118032525613\n",
      "Loss: 0.026102961444442144\n",
      "Loss: 0.02605397155559124\n",
      "Loss: 0.026005147568568796\n",
      "Loss: 0.025956488690826875\n",
      "Loss: 0.025907994134638332\n",
      "Loss: 0.025859663117061275\n",
      "Loss: 0.025811494859904444\n",
      "Loss: 0.025763488589692317\n",
      "Loss: 0.025715643537630662\n",
      "Loss: 0.02566795893957289\n",
      "Loss: 0.025620434035985684\n",
      "Loss: 0.02557306807191617\n",
      "Loss: 0.025525860296958098\n",
      "Loss: 0.025478809965219125\n",
      "Loss: 0.025431916335288286\n",
      "Loss: 0.02538517867020357\n",
      "Loss: 0.025338596237419436\n",
      "Loss: 0.025292168308775585\n",
      "Loss: 0.025245894160464865\n",
      "Loss: 0.025199773073002044\n",
      "Loss: 0.025153804331193037\n",
      "Loss: 0.025107987224103817\n",
      "Loss: 0.025062321045030016\n",
      "Loss: 0.025016805091466518\n",
      "Loss: 0.02497143866507743\n",
      "Loss: 0.024926221071666567\n",
      "Loss: 0.024881151621147426\n",
      "Loss: 0.02483622962751435\n",
      "Loss: 0.024791454408813314\n",
      "Loss: 0.024746825287112865\n",
      "Loss: 0.0247023415884759\n",
      "Loss: 0.0246580026429312\n",
      "Loss: 0.024613807784445222\n",
      "Loss: 0.024569756350894326\n",
      "Loss: 0.024525847684037178\n",
      "Loss: 0.02448208112948707\n",
      "Loss: 0.024438456036685218\n",
      "Loss: 0.024394971758873056\n",
      "Loss: 0.02435162765306633\n",
      "Loss: 0.0243084230800278\n",
      "Loss: 0.024265357404241485\n",
      "Loss: 0.024222429993886182\n",
      "Loss: 0.024179640220809793\n",
      "Loss: 0.02413698746050354\n",
      "Loss: 0.024094471092076623\n",
      "Loss: 0.024052090498230785\n",
      "Loss: 0.024009845065235333\n",
      "Loss: 0.023967734182902187\n",
      "Loss: 0.023925757244561502\n",
      "Loss: 0.023883913647036972\n",
      "Loss: 0.02384220279062149\n",
      "Loss: 0.023800624079053312\n",
      "Loss: 0.02375917691949209\n",
      "Loss: 0.023717860722495286\n",
      "Loss: 0.023676674901994352\n",
      "Loss: 0.02363561887527194\n",
      "Loss: 0.02359469206293838\n",
      "Loss: 0.023553893888908897\n",
      "Loss: 0.023513223780380867\n",
      "Loss: 0.023472681167811423\n",
      "Loss: 0.02343226548489468\n",
      "Loss: 0.0233919761685399\n",
      "Loss: 0.023351812658849364\n",
      "Loss: 0.023311774399096305\n",
      "Loss: 0.023271860835703626\n",
      "Loss: 0.023232071418222054\n",
      "Loss: 0.023192405599309\n",
      "Loss: 0.023152862834707345\n",
      "Loss: 0.023113442583224453\n",
      "Loss: 0.023074144306711203\n",
      "Loss: 0.0230349674700416\n",
      "Loss: 0.022995911541091974\n",
      "Loss: 0.02295697599072093\n",
      "Loss: 0.022918160292748943\n",
      "Loss: 0.02287946392393841\n",
      "Loss: 0.022840886363973842\n",
      "Loss: 0.022802427095442085\n",
      "Loss: 0.022764085603812666\n",
      "Loss: 0.022725861377418766\n",
      "Loss: 0.02268775390743753\n",
      "Loss: 0.02264976268787125\n",
      "Loss: 0.02261188721552835\n",
      "Loss: 0.022574126990004367\n",
      "Loss: 0.022536481513663848\n",
      "Loss: 0.02249895029162116\n",
      "Loss: 0.022461532831722617\n",
      "Loss: 0.022424228644528144\n",
      "Loss: 0.022387037243293044\n",
      "Loss: 0.022349958143950167\n",
      "Loss: 0.02231299086509224\n",
      "Loss: 0.022276134927953932\n",
      "Loss: 0.0222393898563945\n",
      "Loss: 0.02220275517688048\n",
      "Loss: 0.02216623041846817\n",
      "Loss: 0.02212981511278671\n",
      "Loss: 0.02209350879402107\n",
      "Loss: 0.022057310998895106\n",
      "Loss: 0.022021221266654862\n",
      "Loss: 0.021985239139052083\n",
      "Loss: 0.021949364160327604\n",
      "Loss: 0.021913595877194856\n",
      "Loss: 0.021877933838824106\n",
      "Loss: 0.021842377596825883\n",
      "Loss: 0.02180692670523523\n",
      "Loss: 0.021771580720495818\n",
      "Loss: 0.021736339201444188\n",
      "Loss: 0.02170120170929424\n",
      "Loss: 0.021666167807621477\n",
      "Loss: 0.021631237062347933\n",
      "Loss: 0.021596409041726692\n",
      "Loss: 0.021561683316326935\n",
      "Loss: 0.02152705945901863\n",
      "Loss: 0.021492537044957984\n",
      "Loss: 0.021458115651572202\n",
      "Loss: 0.021423794858545266\n",
      "Loss: 0.02138957424780293\n",
      "Loss: 0.021355453403498578\n",
      "Loss: 0.02132143191199863\n",
      "Loss: 0.021287509361868337\n",
      "Loss: 0.021253685343857767\n",
      "Loss: 0.021219959450887657\n",
      "Loss: 0.021186331278035438\n",
      "Loss: 0.021152800422521344\n",
      "Loss: 0.02111936648369508\n",
      "Loss: 0.02108602906302157\n",
      "Loss: 0.02105278776406808\n",
      "Loss: 0.02101964219249021\n",
      "Loss: 0.020986591956018968\n",
      "Loss: 0.020953636664447408\n",
      "Loss: 0.020920775929617552\n",
      "Loss: 0.020888009365407224\n",
      "Loss: 0.02085533658771725\n",
      "Loss: 0.02082275721445867\n",
      "Loss: 0.02079027086553985\n",
      "Loss: 0.020757877162853804\n",
      "Loss: 0.020725575730265962\n",
      "Loss: 0.02069336619360127\n",
      "Loss: 0.020661248180632127\n",
      "Loss: 0.020629221321066045\n",
      "Loss: 0.020597285246533384\n",
      "Loss: 0.020565439590575302\n",
      "Loss: 0.02053368398863175\n",
      "Loss: 0.02050201807802957\n",
      "Loss: 0.020470441497970594\n",
      "Loss: 0.02043895388952\n",
      "Loss: 0.020407554895594482\n",
      "Loss: 0.02037624416095094\n",
      "Loss: 0.020345021332174647\n",
      "Loss: 0.020313886057668255\n",
      "Loss: 0.02028283798764006\n",
      "Loss: 0.02025187677409301\n",
      "Loss: 0.02022100207081344\n",
      "Loss: 0.020190213533360056\n",
      "Loss: 0.020159510819052994\n",
      "Loss: 0.0201288935869627\n",
      "Loss: 0.0200983614978992\n",
      "Loss: 0.020067914214401458\n",
      "Loss: 0.02003755140072645\n",
      "Loss: 0.020007272722838847\n",
      "Loss: 0.01997707784840011\n",
      "Loss: 0.019946966446758523\n",
      "Loss: 0.01991693818893824\n",
      "Loss: 0.01988699274762955\n",
      "Loss: 0.019857129797178227\n",
      "Loss: 0.01982734901357557\n",
      "Loss: 0.019797650074448267\n",
      "Loss: 0.019768032659048362\n",
      "Loss: 0.019738496448243468\n",
      "Loss: 0.0197090411245066\n",
      "Loss: 0.01967966637190665\n",
      "Loss: 0.019650371876098565\n",
      "Loss: 0.01962115732431358\n",
      "Loss: 0.019592022405349928\n",
      "Loss: 0.01956296680956285\n",
      "Loss: 0.019533990228855668\n",
      "Loss: 0.019505092356669865\n",
      "Loss: 0.019476272887976365\n",
      "Loss: 0.019447531519265638\n",
      "Loss: 0.01941886794853899\n",
      "Loss: 0.0193902818752992\n",
      "Loss: 0.01936177300054161\n",
      "Loss: 0.019333341026744916\n",
      "Loss: 0.0193049856578625\n",
      "Loss: 0.019276706599313364\n",
      "Loss: 0.019248503557973386\n",
      "Loss: 0.01922037624216652\n",
      "Loss: 0.01919232436165631\n",
      "Loss: 0.019164347627636864\n",
      "Loss: 0.019136445752724905\n",
      "Loss: 0.019108618450950583\n",
      "Loss: 0.01908086543774955\n",
      "Loss: 0.0190531864299543\n",
      "Loss: 0.019025581145785977\n",
      "Loss: 0.018998049304845954\n",
      "Loss: 0.01897059062810789\n",
      "Loss: 0.018943204837909337\n",
      "Loss: 0.01891589165794361\n",
      "Loss: 0.018888650813252025\n",
      "Loss: 0.018861482030215697\n",
      "Loss: 0.01883438503654752\n",
      "Loss: 0.018807359561284695\n",
      "Loss: 0.01878040533478039\n",
      "Loss: 0.018753522088696368\n",
      "Loss: 0.018726709555995123\n",
      "Loss: 0.0186999674709322\n",
      "Loss: 0.018673295569048627\n",
      "Loss: 0.018646693587163435\n",
      "Loss: 0.018620161263365978\n",
      "Loss: 0.01859369833700861\n",
      "Loss: 0.018567304548699332\n",
      "Loss: 0.018540979640294265\n",
      "Loss: 0.01851472335489056\n",
      "Loss: 0.018488535436818932\n",
      "Loss: 0.018462415631636673\n",
      "Loss: 0.01843636368612043\n",
      "Loss: 0.018410379348258975\n",
      "Loss: 0.018384462367246276\n",
      "Loss: 0.018358612493474653\n",
      "Loss: 0.01833282947852739\n",
      "Loss: 0.01830711307517222\n",
      "Loss: 0.01828146303735435\n",
      "Loss: 0.018255879120189528\n",
      "Loss: 0.018230361079957413\n",
      "Loss: 0.018204908674094834\n",
      "Loss: 0.01817952166118902\n",
      "Loss: 0.018154199800971046\n",
      "Loss: 0.018128942854309257\n",
      "Loss: 0.01810375058320272\n",
      "Loss: 0.0180786227507745\n",
      "Loss: 0.018053559121265713\n",
      "Loss: 0.01802855946002857\n",
      "Loss: 0.018003623533520287\n",
      "Loss: 0.0179787511092968\n",
      "Loss: 0.017953941956006293\n",
      "Loss: 0.017929195843383098\n",
      "Loss: 0.017904512542241482\n",
      "Loss: 0.01787989182446935\n",
      "Loss: 0.017855333463022427\n",
      "Loss: 0.017830837231917706\n",
      "Loss: 0.017806402906228043\n",
      "Loss: 0.017782030262075504\n",
      "Loss: 0.017757719076625923\n",
      "Loss: 0.017733469128082706\n",
      "Loss: 0.017709280195680976\n",
      "Loss: 0.01768515205968184\n",
      "Loss: 0.017661084501366636\n",
      "Loss: 0.0176370773030309\n",
      "Loss: 0.017613130247978885\n",
      "Loss: 0.017589243120517854\n",
      "Loss: 0.017565415705952384\n",
      "Loss: 0.017541647790578572\n",
      "Loss: 0.017517939161678982\n",
      "Loss: 0.017494289607516502\n",
      "Loss: 0.017470698917329167\n",
      "Loss: 0.017447166881324732\n",
      "Loss: 0.017423693290675106\n",
      "Loss: 0.017400277937510967\n",
      "Loss: 0.01737692061491651\n",
      "Loss: 0.01735362111692407\n",
      "Loss: 0.017330379238508702\n",
      "Loss: 0.01730719477558328\n",
      "Loss: 0.01728406752499283\n",
      "Loss: 0.017260997284509694\n",
      "Loss: 0.017237983852828195\n",
      "Loss: 0.017215027029559613\n",
      "Loss: 0.017192126615227038\n",
      "Loss: 0.017169282411260395\n",
      "Loss: 0.01714649421999138\n",
      "Loss: 0.017123761844648394\n",
      "Loss: 0.01710108508935175\n",
      "Loss: 0.01707846375910867\n",
      "Loss: 0.017055897659808365\n",
      "Loss: 0.017033386598217213\n",
      "Loss: 0.017010930381973943\n",
      "Loss: 0.016988528819584868\n",
      "Loss: 0.016966181720419003\n",
      "Loss: 0.016943888894703538\n",
      "Loss: 0.016921650153518878\n",
      "Loss: 0.01689946530879409\n",
      "Loss: 0.016877334173302456\n",
      "Loss: 0.01685525656065651\n",
      "Loss: 0.01683323228530366\n",
      "Loss: 0.01681126116252162\n",
      "Loss: 0.01678934300841385\n",
      "Loss: 0.016767477639905045\n",
      "Loss: 0.01674566487473673\n",
      "Loss: 0.016723904531462806\n",
      "Loss: 0.01670219642944511\n",
      "Loss: 0.016680540388848933\n",
      "Loss: 0.016658936230638938\n",
      "Loss: 0.016637383776574585\n",
      "Loss: 0.01661588284920592\n",
      "Loss: 0.01659443327186943\n",
      "Loss: 0.01657303486868339\n",
      "Loss: 0.016551687464544225\n",
      "Loss: 0.01653039088512184\n",
      "Loss: 0.01650914495685573\n",
      "Loss: 0.016487949506950657\n",
      "Loss: 0.0164668043633727\n",
      "Loss: 0.01644570935484512\n",
      "Loss: 0.016424664310844155\n",
      "Loss: 0.01640366906159529\n",
      "Loss: 0.016382723438068836\n",
      "Loss: 0.016361827271976454\n",
      "Loss: 0.01634098039576665\n",
      "Loss: 0.016320182642621237\n",
      "Loss: 0.016299433846451297\n",
      "Loss: 0.016278733841893347\n",
      "Loss: 0.01625808246430528\n",
      "Loss: 0.016237479549762745\n",
      "Loss: 0.016216924935055256\n",
      "Loss: 0.016196418457682535\n",
      "Loss: 0.016175959955850444\n",
      "Loss: 0.01615554926846742\n",
      "Loss: 0.016135186235140944\n",
      "Loss: 0.01611487069617343\n",
      "Loss: 0.016094602492559028\n",
      "Loss: 0.016074381465979413\n",
      "Loss: 0.016054207458800807\n",
      "Loss: 0.01603408031406979\n",
      "Loss: 0.016013999875510096\n",
      "Loss: 0.01599396598751884\n",
      "Loss: 0.015973978495163143\n",
      "Loss: 0.015954037244176492\n",
      "Loss: 0.015934142080955286\n",
      "Loss: 0.01591429285255536\n",
      "Loss: 0.01589448940668854\n",
      "Loss: 0.015874731591719274\n",
      "Loss: 0.015855019256661107\n",
      "Loss: 0.015835352251173277\n",
      "Loss: 0.01581573042555757\n",
      "Loss: 0.015796153630754668\n",
      "Loss: 0.01577662171834103\n",
      "Loss: 0.01575713454052559\n",
      "Loss: 0.01573769195014628\n",
      "Loss: 0.015718293800667006\n",
      "Loss: 0.015698939946174224\n",
      "Loss: 0.01567963024137382\n",
      "Loss: 0.015660364541587757\n"
     ]
    }
   ],
   "source": [
    "input_size = 3\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "learning_rate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "nn = Model(input_size, hidden_size, output_size)\n",
    "\n",
    "# Sample input and target\n",
    "input_data = np.array([0.5, 0.2, 0.1])\n",
    "target_data = np.array([1])\n",
    "\n",
    "# Train the model\n",
    "Training.train(nn, input_data, target_data, epochs, learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
