{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "510b2366",
   "metadata": {},
   "source": [
    "#### linear, ReLU, sigmoid, tanh, and softmax activation functions as a class for neural networks implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21015e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU: [0 0 1]\n",
      "Sigmoid: [0.26894142 0.5        0.73105858]\n",
      "Tanh: [-0.76159416  0.          0.76159416]\n",
      "Softmax: [[0.26894142 0.26894142 0.26894142]\n",
      " [0.73105858 0.73105858 0.73105858]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinearActivation:\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class ReLUActivation:\n",
    "    def forward(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "class SigmoidActivation:\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class TanhActivation:\n",
    "    def forward(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "class SoftmaxActivation:\n",
    "    def forward(self, x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "\n",
    "# Example usage\n",
    "x = np.array([-1, 0, 1])\n",
    "\n",
    "relu = ReLUActivation()\n",
    "print(\"ReLU:\", relu.forward(x))\n",
    "\n",
    "sigmoid = SigmoidActivation()\n",
    "print(\"Sigmoid:\", sigmoid.forward(x))\n",
    "\n",
    "tanh = TanhActivation()\n",
    "print(\"Tanh:\", tanh.forward(x))\n",
    "\n",
    "softmax = SoftmaxActivation()\n",
    "print(\"Softmax:\", softmax.forward(np.array([x, x+1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac48eb87",
   "metadata": {},
   "source": [
    "#### class structure and forward propagation including the loss (cost) function implementation for a deep (multilayer) neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32ff82ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1008788497199111\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation Function Classes\n",
    "class LinearActivation:\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "    def backward(self, x):\n",
    "        return np.ones_like(x)\n",
    "\n",
    "class ReLUActivation:\n",
    "    def forward(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "class SigmoidActivation:\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def backward(self, x):\n",
    "        sigmoid = self.forward(x)\n",
    "        return sigmoid * (1 - sigmoid)\n",
    "\n",
    "class TanhActivation:\n",
    "    def forward(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def backward(self, x):\n",
    "        return 1 - np.tanh(x)**2\n",
    "\n",
    "class SoftmaxActivation:\n",
    "    def forward(self, x):\n",
    "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "    def backward(self, x):\n",
    "        softmax = self.forward(x)\n",
    "        return softmax * (1 - softmax)\n",
    "\n",
    "# Layer Class\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, activation_function):\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.1\n",
    "        self.biases = np.zeros((1, output_size))\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.z = np.dot(inputs, self.weights) + self.biases\n",
    "        self.output = self.activation_function.forward(self.z)\n",
    "        return self.output\n",
    "\n",
    "# Neural Network Class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        for layer in self.layers:\n",
    "            inputs = layer.forward(inputs)\n",
    "        return inputs\n",
    "\n",
    "    def calculate_loss(self, y_pred, y_true):\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "# Example usage\n",
    "nn = NeuralNetwork()\n",
    "nn.add_layer(Layer(3, 5, ReLUActivation()))\n",
    "nn.add_layer(Layer(5, 2, SigmoidActivation()))\n",
    "\n",
    "# Dummy input data and true values for demonstration\n",
    "input_data = np.random.randn(10, 3)\n",
    "y_true = np.random.randn(10, 2)\n",
    "\n",
    "# Forward propagation\n",
    "output = nn.forward(input_data)\n",
    "\n",
    "# Loss calculation\n",
    "loss = nn.calculate_loss(output, y_true)\n",
    "print(f\"Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cf2b08",
   "metadata": {},
   "source": [
    "#### Backpropagation implementation for a deep (multilayer) neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ab25453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.2852148925503146\n",
      "Epoch 100, Loss: 1.260299178517058\n",
      "Epoch 200, Loss: 1.2386443084347583\n",
      "Epoch 300, Loss: 1.219975240445666\n",
      "Epoch 400, Loss: 1.203876631933929\n",
      "Epoch 500, Loss: 1.1900246640992325\n",
      "Epoch 600, Loss: 1.178113502128264\n",
      "Epoch 700, Loss: 1.1678628292997772\n",
      "Epoch 800, Loss: 1.1590092790405258\n",
      "Epoch 900, Loss: 1.1513091616649012\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation Function Classes (with backward methods)\n",
    "class LinearActivation:\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout\n",
    "\n",
    "class ReLUActivation:\n",
    "    def forward(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, dout, inputs):\n",
    "        return dout * (inputs > 0)\n",
    "\n",
    "class SigmoidActivation:\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def backward(self, dout, inputs):\n",
    "        sigmoid = self.forward(inputs)\n",
    "        return dout * sigmoid * (1 - sigmoid)\n",
    "\n",
    "class TanhActivation:\n",
    "    def forward(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def backward(self, dout, inputs):\n",
    "        return dout * (1 - np.tanh(inputs)**2)\n",
    "\n",
    "class SoftmaxActivation:\n",
    "    def forward(self, x):\n",
    "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "    def backward(self, dout, inputs):\n",
    "        softmax = self.forward(inputs)\n",
    "        return dout * softmax * (1 - softmax)\n",
    "\n",
    "# Layer Class (with backward method)\n",
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, activation_function):\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.1\n",
    "        self.biases = np.zeros((1, output_size))\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.z = np.dot(inputs, self.weights) + self.biases\n",
    "        self.output = self.activation_function.forward(self.z)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dactivation = self.activation_function.backward(dout, self.z)\n",
    "        self.dweights = np.dot(self.inputs.T, dactivation)\n",
    "        self.dbiases = np.sum(dactivation, axis=0, keepdims=True)\n",
    "        return np.dot(dactivation, self.weights.T)\n",
    "\n",
    "# Neural Network Class (with update method for backpropagation)\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.layers = []\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        for layer in self.layers:\n",
    "            inputs = layer.forward(inputs)\n",
    "        return inputs\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        # Loss derivative (assuming MSE loss)\n",
    "        dout = 2 * (y_pred - y_true) / y_true.size\n",
    "\n",
    "        # Backpropagation\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "    def update_weights(self):\n",
    "        for layer in self.layers:\n",
    "            layer.weights -= self.learning_rate * layer.dweights\n",
    "            layer.biases -= self.learning_rate * layer.dbiases\n",
    "\n",
    "    def train(self, x_train, y_train, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(x_train)\n",
    "            self.backward(output, y_train)\n",
    "            self.update_weights()\n",
    "            if epoch % 100 == 0:  # Print loss every 100 epochs\n",
    "                loss = np.mean((output - y_train) ** 2)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Example usage\n",
    "nn = NeuralNetwork(learning_rate=0.01)\n",
    "nn.add_layer(Layer(3, 5, ReLUActivation()))\n",
    "nn.add_layer(Layer(5, 2, SigmoidActivation()))\n",
    "\n",
    "# Dummy training data\n",
    "x_train = np.random.randn(100, 3)\n",
    "y_train = np.random.randn(100, 2)\n",
    "\n",
    "# Training the network\n",
    "nn.train(x_train, y_train, epochs=1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
