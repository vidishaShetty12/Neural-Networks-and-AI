# Neural-Networks-and-AI
## Brain, Neurons, and Models
#### This code effectively simulates a basic McCulloch-Pitts neuron, showing how it processes input and generates a binary output based on the given threshold and weights. This forms a fundamental part of neural network computations
### Fitting, Bias, Regularization, and Dropout
#### The provided code is a Python script using TensorFlow to create a neural network for classifying the MNIST dataset, which consists of handwritten digits. The network architecture includes two dense layers with ReLU activation, interspersed with dropout layers for regularization. Dropout rates of 50% and 30% are applied after the first and second dense layers, respectively, to prevent overfitting by randomly dropping neuron connections during training. L2 regularization is also implemented in the dense layers to further control overfitting. The model is compiled with the Adam optimizer and sparse categorical crossentropy loss function. It is trained and validated on the MNIST dataset. Finally, the model's accuracy is evaluated on the test set.
